\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsfonts, amsmath, amssymb, MnSymbol, graphicx, algpseudocode, algorithm, amsthm}
\usepackage{hyperref}

%%% Helpers
\newcommand{\algtab}{\hspace{\algorithmicindent}}
\newcommand{\wi}{\omega_i}
\newcommand{\hsigma}{\widehat \Sigma}
\newcommand{\hmu}{\hat \mu}

%%% Shortcuts
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bm}{\mathbf{\mu}}
\newcommand{\bsig}{\mathbf{\Sigma}}

%%% Draft helpers (remember to remove when done)
\newcommand{\outline}[2]{\paragraph{\textsc{#1}}\hrulefill~\\{\small\it #2}\\\_\hrulefill~\\}
\newcommand{\todo}[1]{\outline{\large TODO}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{
    CAP5638 Project 1\\
    \large Classification Using Maximum-likelihood, Parzen Window, and k-Nearest Neighbor
}
\author{Suhib Sam Kiswani}
\date{October 26, 2015}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\todo{You need to turn in a report, including description of your algorithms and implementations,
experimental results, and comparison and evaluation and analysis of the three different methods.}

\abstract{For each dataset, you need to first estimate the necessary data normalization and parameters using the training set only (you need to document the results for different choices of parameters on the training set in the leaveone- out sense for Parzen windows and k-nearest neighbor rule) and then document the classification results on the test set using each of the methods. Then you need to compare different methods in terms of classification performance and required time for classification, and give justifications for your observed empirical results.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Details}

The algorithms were implemented in {\it Python 3.4}, with a dependence on the \textit{scipy} \cite{sp} library.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Maximum likelihood estimation}
\todo{You need to choose proper parametric forms for the underlying probability distributions. In the report, you need to specify the parametric forms, the maximum-likelihood estimate for the parametric forms, and the resulting discriminant functions for classification for each dataset.}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Normal Density}
The discriminant function for the normal density is:

$$g_i(\bx) = \bx^T\mathbf{W}_i\bx + \mathbf{w}_i^T \bx + w_{i0}$$
where:
$$\mathbf{W}_i = -0.5 \bsig_i^{-1}$$
$$\mathbf{w}_i = \bsig_i^{-1} \bm_i$$
$$w_{i0} = -0.5 \bm_i^T \bsig_i^{-1} \bm_i - 0.5\ln \left( \det \bsig_i \right) + \ln P(\omega_i)$$

Using the training samples, the mean and covariance can be estimated with maximum likelihood using the following definitions:
\begin{align*}
\hat\bm_i = \frac{1}{n} \sum^n_{k=1} \bx_k &~& \widehat\bsig_i = \frac{1}{n} \sum^n_{k=1} (\bx_k - \hat\bm_i)(\bx_k - \hat\bm_i)^T
\end{align*}

Which yields:
$$p(\bx|\mathbf{\omega_i}) = \frac{1}{\sqrt{(2 \pi)^n \det \widehat\bsig_i}} \exp\left( -\frac{1}{2} (\bx - \hat\bm_i)^T \widehat\bsig_i^{-1} (\bx - \hat\bm_i) \right)$$

The classification of instance $\bx$ is $\mathbf{\omega_i} = \arg\max_{\mathbf{\omega_i}} \left| p(\bx|\mathbf{\omega_i}) P(\mathbf{\omega_i}) \right|$

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Uniform}
\todo{write out eqns}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Parzen window estimation}
\todo{You need to choose proper window functions, which you need to specify in the report along with the resulting discriminant functions for classification for each dataset. Here we choose the parameter values using the leave-one-out performance on the training set: For a set of candidate values, we compute the leave-one-out performance on the training set for each candidate and the optimal one is the one that gives the best leave-one-out performance (in case there are ties, specify how the ties will be broken).}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Basic k-nearest neighbor}
\todo{In this case, you need to implement the k-nearest neighbor classifier by first finding the first k nearest neighbors of an input and then classifying it as the class that appeared most among the nearest neighbors. Here you need to specify the distance measure between two inputs. Here we choose the parameter values using the leave-one-out performance on the training set: For a set of candidate values, we compute the leave-one-out performance on the training set for each candidate and the optimal one is the one that gives the best leave-one-out performance (in case there are ties, specify how the ties will be broken).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}
\todo{Document the classification results on the test set using each of the methods. For each dataset, you need to first estimate the necessary data normalization and parameters using the training set only (you need to document the results for different choices of parameters on the training set in the leaveone- out sense for Parzen windows and k-nearest neighbor rule)}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Iris Data Set}

\subsubsection{Maximum likelihood estimation}
\begin{enumerate}
\item {\bf Normal Density}
The estimated parameters of $\hat\theta$ for each of the classes from the training samples were:
$$\hat\bm_1 = \left( 4.98181818,  3.39090909,  1.45151515,  0.25151515 \right)$$
$$\widehat\bsig_1 = \begin{pmatrix}
    0.10876033 & 0.08619835 & 0.02033058 & 0.01093664\\
    0.08619835 & 0.13597796 & 0.01410468 & 0.00865014\\
    0.02033058 & 0.01410468 & 0.03401286 & 0.00825528\\
    0.01093664 & 0.00865014 & 0.00825528 & 0.0134068
\end{pmatrix}$$

$$\hat\bm_2 = \left( 5.89090909,  2.78787879,  4.26363636,  1.31515152 \right)$$
$$\widehat\bsig_2 = \begin{pmatrix}
    0.2353719  & 0.06867769 & 0.165427   & 0.05256198\\
    0.06867769 & 0.08530762 & 0.07743802 & 0.04442608\\
    0.165427   & 0.07743802 & 0.20110193 & 0.06933884\\
    0.05256198 & 0.04442608 & 0.06933884 & 0.0394674
\end{pmatrix}$$

$$\hat\bm_3 = \left( 6.66060606,  2.94848485,  5.58484848,  1.99393939 \right)$$
$$\widehat\bsig_3 = \begin{pmatrix}
    0.36359963 & 0.11887971 & 0.2851607  & 0.03976125\\
    0.11887971 & 0.10613407 & 0.08861341 & 0.03817264\\
    0.2851607  & 0.08861341 & 0.29219467 & 0.03202938\\
    0.03976125 & 0.03817264 & 0.03202938 & 0.06784206
\end{pmatrix}$$

\smallskip

This method correctly classified $48$ of the $51$ testing samples ($94.1$\% accuracy).

\todo{add decision boundary graphs}

\item {\bf Uniform}
\todo{gotta do it}

\end{enumerate}

\subsubsection{Parzen window estimation}
\todo{document results for diff params on training set in the leaveone-out sense and document results.}

\subsubsection{Basic k-nearest neighbor}
\todo{you need to first estimate the necessary data normalization and parameters using the training set only (you need to document the results for different choices of parameters on the training set in the leaveone-out sense for Parzen windows and k-nearest neighbor rule) and then document the classification results on
the test set using each of the methods.}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{UCI Wine Data Set}
\todo{you need to first estimate the necessary data normalization and parameters using the training set only (you need to document the results for different choices of parameters on the training set in the leaveone-out sense for Parzen windows and k-nearest neighbor rule) and then document the classification results on
the test set using each of the methods.}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Handwritten Digits Data Set}
\todo{you need to first estimate the necessary data normalization and parameters using the training set only (you need to document the results for different choices of parameters on the training set in the leaveone-out sense for Parzen windows and k-nearest neighbor rule) and then document the classification results on
the test set using each of the methods.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis}
\todo{You need to compare different methods in terms of classification performance and required time for classification, and give justifications for your observed empirical results.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extra Credit}
\todo{Please state clearly in your report if you have implemented any of the following extra
credit options.}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Speed up using k-d tree}
\todo{With k from 1 to 10 with an increment of 1, first build a k-d tree from the training set and then classify the test samples using the k-nearest neighbor classifier by finding the nearest neighbors using the k-d tree. Compare the classification accuracy and the number of distance calculations with the basic k nearest neighbor implementation on the three datasets. Summarize your observations and justify your results.}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsection{Recognition of my handwritten digits}
\todo{Apply the best classifier you have for hand written digit recognition on a test set consisting of your own written digits (you need to create the dataset). Document the classification performance, what you have done to improve the performance, and any additional issues you have handled.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}

\bibitem{sp}
    Jones E, Oliphant E, Peterson P, \emph{et al.}
    {\bf SciPy: Open Source Scientific Tools for Python}, 2001-,
    \url{http://www.scipy.org/} [Online; accessed 2015-10-24].

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}